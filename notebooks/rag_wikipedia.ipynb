{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1386ff-64ab-470c-b8b3-b0bbe8bebc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero descargamos algunas dependencias necesarias:\n",
    "!pip install -q wikipedia-api sentence-transformers chromadb langchain langchain-community langchain-chroma transformers torch pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac3176-2b04-4668-84b3-0a8955e5565e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched page: Federated learning — length 31699 characters\n",
      "CSV saved at: \\mnt\\data\\wiki_corpus.csv\n"
     ]
    }
   ],
   "source": [
    "#Hacemos unas dependencias:\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import wikipediaapi\n",
    "\n",
    "# Wikipedia API (con User-Agent válido):\n",
    "wiki_client = wikipediaapi.Wikipedia(\n",
    "    language=\"en\",\n",
    "    user_agent=\"ResearchAgentJQ/1.0 (contact: joaquin@example.com)\"\n",
    ")\n",
    "\n",
    "# Fetch page:\n",
    "article = wiki_client.page(\"Federated_learning\")\n",
    "full_text = article.text\n",
    "print(f\"Downloaded: {article.title} | Characters: {len(full_text)}\")\n",
    "\n",
    "# Función de chunking:\n",
    "def split_into_chunks(content, size=300):\n",
    "    tokens = content.split()\n",
    "    segmented = []\n",
    "    index = 0\n",
    "\n",
    "    while index < len(tokens):\n",
    "        segmented.append(\" \".join(tokens[index:index + size]))\n",
    "        index += size\n",
    "\n",
    "    return segmented\n",
    "\n",
    "\n",
    "segments = split_into_chunks(full_text, 300)\n",
    "\n",
    "# Guardamos los chunks en un csv:\n",
    "output_file = Path(\"/mnt/data/wiki_corpus.csv\")\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with output_file.open(\"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "    fields = [\"id\", \"title\", \"text\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for n, segment in enumerate(segments):\n",
    "        writer.writerow({\n",
    "            \"id\": f\"fed_chunk_{n}\",\n",
    "            \"title\": article.title,\n",
    "            \"text\": segment\n",
    "        })\n",
    "\n",
    "print(f\"csv saved at: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e733999-f231-4c37-9b13-127acd7e4569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be06b1a0e693433188e67684f83c236f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 15 chunks to ChromaDB\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "df = pd.read_csv('/mnt/data/data/wiki_corpus.csv')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "#Creamos el cliente Chroma y la collección:\n",
    "\n",
    "client = chromadb.Client(Settings())\n",
    "collection = client.create_collection('wiki_ai')\n",
    "collection.add(ids=df['id'].tolist(), metadatas=[{'title':t} for t in df['title'].tolist()], documents=df['text'].tolist(), embeddings=embeddings.tolist())\n",
    "print('Upserted', len(df), 'chunks to ChromaDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338dcee3-a9df-42ed-be95-1d1239dbbd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajoaq\\AppData\\Local\\Temp\\ipykernel_31300\\2283568630.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb_fn = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain listo (sin langchain.chains)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajoaq\\AppData\\Local\\Temp\\ipykernel_31300\\2283568630.py:23: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\", temperature=0.3)\n"
     ]
    }
   ],
   "source": [
    "# RAG Pipeline \n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 1. Embeddings\n",
    "emb_fn = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Vectorstore\n",
    "db = Chroma(\n",
    "    persist_directory=\"/mnt/data/chroma_db\",\n",
    "    collection_name=\"wiki_ai\",\n",
    "    embedding_function=emb_fn\n",
    ")\n",
    "\n",
    "# 3. Retriever\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 4. LLM\n",
    "llm = Ollama(model=\"mistral\", temperature=0.3)\n",
    "\n",
    "# 5. Prompt personalizado\n",
    "template = \"\"\"You are an expert in federated learning. Use only the following context to answer the question.\n",
    "If the context doesn't contain the answer, say \"I don't know based on the provided data.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer (detailed, 400–500 words):\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# RAG Chain (SIN langchain.chains)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38754449-1906-4906-8ebf-1fe72c092b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos rag_summary.md (400–500 palabras)\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "query = \"Explain federated learning challenges in healthcare.\"\n",
    "\n",
    "print(f\"Consultando: '{query}'\")\n",
    "raw_docs = retriever.invoke(query)\n",
    "context = format_docs(raw_docs)\n",
    "answer = rag_chain.invoke(query)\n",
    "\n",
    "# Asegurar longitud\n",
    "word_count = len(answer.split())\n",
    "if word_count < 400:\n",
    "    print(f\"Ampliando respuesta ({word_count} palabras)...\")\n",
    "    extended_query = \"Provide a detailed 450-word summary of federated learning challenges in healthcare, including privacy, data heterogeneity, and regulatory issues.\"\n",
    "    answer = rag_chain.invoke(extended_query)\n",
    "\n",
    "# Guardar resumen\n",
    "summary_md = f\"\"\"# RAG Summary: Federated Learning Challenges in Healthcare\n",
    "\n",
    "**Query**: {query}  \n",
    "**Word Count**: {len(answer.split())}  \n",
    "**Retrieved Chunks**: {len(raw_docs)}  \n",
    "**Generated on**: November 16, 2025\n",
    "\n",
    "---\n",
    "\n",
    "{answer}\n",
    "\n",
    "---\n",
    "\n",
    "*Source: Wikipedia via RAG (Manual Chain, ChromaDB, Ollama-Mistral)*  \n",
    "*No se usó `langchain.chains`*\n",
    "\"\"\"\n",
    "\n",
    "output_dir = Path(\"/mnt/data/outputs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "summary_path = output_dir / \"rag_summary.md\"\n",
    "summary_path.write_text(summary_md, encoding='utf-8')\n",
    "print(f\"Resumen guardado: {summary_path}\")\n",
    "\n",
    "# Guardar retrieval\n",
    "retrieval_examples = [\n",
    "    {\n",
    "        \"chunk_id\": doc.metadata.get(\"id\", \"unknown\"),\n",
    "        \"title\": doc.metadata.get(\"title\", \"Federated Learning\"),\n",
    "        \"text_preview\": doc.page_content[:200] + \"...\"\n",
    "    }\n",
    "    for doc in raw_docs\n",
    "]\n",
    "\n",
    "examples_path = output_dir / \"retrieval_examples.json\"\n",
    "examples_path.write_text(json.dumps(retrieval_examples, indent=2), encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd375354-eed8-473c-96a5-d06ae41502f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
